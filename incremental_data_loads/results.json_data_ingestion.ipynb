{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dc0e84b-c027-4efb-a5cf-ba4c06353a8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"p_data_source\", \"\")\n",
    "v_data_source = dbutils.widgets.get(\"p_data_source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a646b3f4-030c-4127-9047-8edb9baaf34a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"p_file_date\", \"\")\n",
    "v_file_date = dbutils.widgets.get(\"p_file_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "225037a7-eeed-481c-b91a-185752908fd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../includes/paths\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "364b23b2-8f3a-4430-a963-41bd677d5c26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../includes/common_functions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1847d65-c706-4409-98df-e1a31ed49051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "results_schema = StructType([StructField(\"constructorId\", IntegerType(), True),\n",
    "                            StructField(\"driverId\", IntegerType(), True),\n",
    "                            StructField(\"fastestLap\", IntegerType(), True),\n",
    "                            StructField(\"fastestLapSpeed\", StringType(), True),\n",
    "                            StructField(\"fastestLapTime\", StringType(), True),\n",
    "                            StructField(\"grid\", IntegerType(), True),\n",
    "                            StructField(\"laps\", IntegerType(), True),\n",
    "                            StructField(\"milliseconds\", IntegerType(), True),\n",
    "                            StructField(\"number\", IntegerType(), True),\n",
    "                            StructField(\"points\", FloatType(), True),\n",
    "                            StructField(\"position\", IntegerType(), True),\n",
    "                            StructField(\"positionOrder\", IntegerType(), True),\n",
    "                            StructField(\"positionText\", StringType(), True),\n",
    "                            StructField(\"raceId\", IntegerType(), True),\n",
    "                            StructField(\"rank\", IntegerType(), True),\n",
    "                            StructField(\"resultId\", IntegerType(), True),\n",
    "                            StructField(\"statusId\", IntegerType(), True),\n",
    "                            StructField(\"time\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fe6040d-0250-4d2b-86e7-6b22fbd96346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df = spark.read.schema(results_schema).json(f\"{raw_container_folder_path}/{v_file_date}/results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f770150-95b5-47b1-a974-fa201c318077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "results_final_df  = results_df.withColumnRenamed(\"constructorId\", \"constructor_id\")\\\n",
    "                                .withColumnRenamed(\"driverId\", \"driver_id\")\\\n",
    "                                .withColumnRenamed(\"fastestLap\", \"fastest_lap\")\\\n",
    "                                .withColumnRenamed(\"fastestLapSpeed\", \"fastest_lap_speed\")\\\n",
    "                                .withColumnRenamed(\"fastestLapTime\", \"fastest_lap_time\")\\\n",
    "                                .withColumnRenamed(\"positionText\", \"position_text\")\\\n",
    "                                .withColumnRenamed(\"raceId\", \"race_id\")\\\n",
    "                                .withColumnRenamed(\"resultId\", \"result_id\")\\\n",
    "                                .withColumnRenamed(\"positionOrder\", \"position_order\") \\\n",
    "                                .withColumn(\"ingestion_date\",current_timestamp())\\\n",
    "                                .withColumn(\"data_source\", lit(v_data_source))\\\n",
    "                                .withColumn(\"file_date\", lit(v_file_date))\\\n",
    "                                .drop(\"statusId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16a7de89-f087-4904-bba5-3c342da8e43b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_no_dup = results_final_df.dropDuplicates([\"race_id\",\"driver_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dab139ba-f3b7-4725-b3fe-be4a19421346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #incremental load method 1 - with append mode. \n",
    "# # Repair the table to update the Hive metastore with partition information\n",
    "# #spark.sql(\"MSCK REPAIR TABLE f1_processed_db.results\")\n",
    "\n",
    "# #iterating through the list of distinct race_id present in final df\n",
    "# for race_id_list in results_final_df.select(\"race_id\").distinct().collect(): \n",
    "#     #checks if the table exists, command will not run on first run as table will be there so this can avoid any unwanted errors\n",
    "#     if (spark._jsparkSession.catalog().tableExists(\"f1_processed_db.results\")): \n",
    "#         #this checks for the partation col ie race id, if the race id exists it will drop it, then as per code in next cell new partation will be created and data will be loaded \n",
    "#         spark.sql(f\"ALTER TABLE f1_processed_db.results DROP If exists PARTITION (race_id = {race_id_list.race_id})\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f25148c-82d8-4761-a858-1ae19b0f6a1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#method 2 - using overwrite mode and insterinto instead of save as table\n",
    "# we use 2 write statements on a normal one which will load the data for the first time then a incremental load using insterInto. ie.. else block runs for fresh run then once table is created if block will be executed\n",
    "# also we need to sepify partation as dynamic\n",
    "\n",
    "# if (spark._jsparkSession.catalog().tableExists(\"f1_processed_db.results\")):\n",
    "#     results_final_df.write.mode(\"overwrite\").insertInto(\"f1_processed_db.results\") # we do not have a option to specify partition in insertInto, by default spark will consider last columnof df as partation column so need to adjust schema so that race is comes last using select in above cell\n",
    "# else:\n",
    "#     results_final_df.write.mode(\"overwrite\").partitionBy(\"race_id\").format(\"parquet\").saveAsTable(\"f1_processed_db.results\")\n",
    "\n",
    "# added this to common function will be calling the function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8f586bd-1b13-4248-b777-1c9018160246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.databricks.optimizer.dynamicPartitionPruning\", \"true\")\n",
    "# from delta.tables import DeltaTable\n",
    "# if (spark._jsparkSession.catalog().tableExists(\"f1_processed_db.results\")):\n",
    "#   deltaTable = DeltaTable.forPath(spark, f\"{processed_container_folder_path}/results\")\n",
    "#   deltaTable.alias(\"target_tab\")\\\n",
    "#     .merge(results_final_df.alias(\"source_tab\"),\\\n",
    "#       \"target_tab.result_id = source_tab.result_id AND target_tab.race_id = source_tab.race_id\")\\\n",
    "#       .whenMatchedUpdateAll()\\\n",
    "#         .whenNotMatchedInsertAll()\\\n",
    "#           .execute()\n",
    "# else:\n",
    "#   results_final_df.write.mode(\"overwrite\").partitionBy(\"race_id\").format(\"delta\").saveAsTable(\"f1_processed_db.results\")\n",
    "\n",
    "# converting the above code to a function so that we can use it in other notebooks function is in commom functions notebook\n",
    "#in the merge condition we have added the partation col check ie target_tab.race_id = source_tab.race_id bcz it will help spark to check for result_id in the partation, this a process of improving the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f5d5c2c-e18d-47fa-b9e2-3f6f39a7c79e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merge_condition  = \"target_tab.result_id = source_tab.result_id AND target_tab.race_id = source_tab.race_id\"\n",
    "merge_incremental_data(result_no_dup,processed_container_folder_path, merge_condition, \"race_id\", \"f1_processed_db\", \"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d820aee0-5366-4c46-bc29-6fdb6e24f624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(f\"{processed_container_folder_path}/results\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e295cf0-d9aa-4a57-a423-dd2af871e965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"success\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "results.json_data_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
