{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56a3cd0c-2e5f-4999-8303-c102c39666c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- has function , add_ingestion date, re_order_df_cols, load_incremental_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "063b6556-e3a2-48bd-836a-0f74f06d92ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "def add_ingestion_date(in_df):\n",
    "    out_df = in_df.withColumn(\"ingestion_date\", current_timestamp())\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a91fc7ec-dc7c-4271-b694-89a780914c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#method 1\n",
    "# Filter the list into two parts: one without 'id' and the other with only 'id'.\n",
    "# my_list = [1, 2, 3, 4, 5, 6, 'id', 8, 9]\n",
    "# Concatenate these two parts, ensuring 'id' is at the end.\n",
    "# output = [1, 2, 3, 4, 5, 6, 8, 9, 'id']\n",
    "\n",
    "def re_order_df_cols(input_df, partation_col):\n",
    "    ordered_cols = []\n",
    "    for col_name in input_df.schema.names:\n",
    "        if col_name != partation_col:\n",
    "            ordered_cols.append(col_name)\n",
    "    ordered_cols.append(partation_col)\n",
    "    output_df = input_df.select(ordered_cols)\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcf195c7-4c3f-46dc-b817-a0074cb011ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function to load incremental data (other methods are available in incremental load> results_ingestion script)\n",
    "# parameters - input_df, partation_col, db_name, table_name\n",
    "def load_incremental_data(input_df, partation_col, db_name, table_name):\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "    ordered_df = re_order_df_cols(input_df, partation_col)\n",
    "    if (spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\")):\n",
    "        ordered_df.write.mode(\"overwrite\").insertInto(f\"{db_name}.{table_name}\") # we do not have a option to specify partition in insertInto, by default spark will consider last columnof df as partation column so need to adjust schema so that race is comes last using select in above cell\n",
    "    else:\n",
    "        ordered_df.write.mode(\"overwrite\").partitionBy(f\"{partation_col}\").format(\"parquet\").saveAsTable(f\"{db_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e7adb58-5830-4249-a873-2ef1f9b3529e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_incremental_data(input_df, folder_path,merge_condition, partation_col, db_name, table_name):\n",
    "    spark.conf.set(\"spark.databricks.optimizer.dynamicPartitionPruning\", \"true\")\n",
    "    from delta.tables import DeltaTable\n",
    "    if (spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\")):\n",
    "        deltaTable = DeltaTable.forPath(spark, f\"{folder_path}/{table_name}\")\n",
    "        deltaTable.alias(\"target_tab\")\\\n",
    "            .merge(input_df.alias(\"source_tab\"),\\\n",
    "            merge_condition)\\\n",
    "            .whenMatchedUpdateAll()\\\n",
    "            .whenNotMatchedInsertAll()\\\n",
    "            .execute()\n",
    "    else:\n",
    "        input_df.write.mode(\"overwrite\").partitionBy(f\"{partation_col}\").format(\"delta\").saveAsTable(f\"{db_name}.{table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "common_functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
